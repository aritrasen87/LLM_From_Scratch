{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPxR+f6+oZFRcHmYOY9dtIO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aritrasen87/LLM_From_Scratch/blob/main/1_LLM_From_Scratch_PytorchEssentials.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytorch Essentials for Buildilng LLM from Scratch - https://www.youtube.com/@AritraSen\n",
        "\n",
        "Deep learning with Pytorch: https://youtube.com/playlist?list=PLOrU905yPYXJsJSHJsiE779KfcrRCgz4v&si=kMm_xrDwfKF9mVCk"
      ],
      "metadata": {
        "id": "iM5-IrX4vVPM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "T5vEp0lMuxLH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsTkSW2uvgOe",
        "outputId": "2145572d-b8c2-47cf-f956-ca115afe0ab2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor1 = torch.randn(4, 5)\n",
        "tensor2 = torch.randn(5, 4)"
      ],
      "metadata": {
        "id": "-3iNlBl7vhSj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor1.shape , tensor2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkAAshU_viec",
        "outputId": "962b5e6e-b426-4828-cfa5-a082f97d10cd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([4, 5]), torch.Size([5, 4]))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tensor1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BM32OEevj97",
        "outputId": "675064ec-4895-47f9-a2b2-8be5cfbd3e3c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.5052,  0.0696,  0.3811, -1.0432, -1.8870],\n",
            "        [-0.3648,  1.0843, -0.5075, -0.6726,  0.6484],\n",
            "        [-1.1391, -0.9684,  0.1949, -0.4022,  0.1019],\n",
            "        [-0.5731,  1.8145,  1.1503,  0.8639, -0.1164]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tensor2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYQGgzTZvlrZ",
        "outputId": "91ea1d38-4f68-4def-cf4d-d37b6de02f35"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.1066,  0.9748, -1.2744, -0.2056],\n",
            "        [-0.9056,  1.1066,  0.1553,  0.4427],\n",
            "        [-1.0844,  0.2307, -0.7648,  1.8045],\n",
            "        [-0.6907, -1.0077,  0.5109, -1.6123],\n",
            "        [-0.0590,  0.1446, -0.5474, -1.9317]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dot Product"
      ],
      "metadata": {
        "id": "mkcJAvCbvpHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor3 = tensor1 @ tensor2\n",
        "print(tensor3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M968D87DvmyD",
        "outputId": "380c4348-79b5-4fa3-cb0b-a665b2333700"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.4095,  0.4509,  0.8631,  6.1494],\n",
            "        [ 0.0336,  1.4988,  0.3229, -0.5290],\n",
            "        [ 1.0589, -1.7170,  0.8910,  0.6088],\n",
            "        [-3.4194,  0.8273,  0.6376,  1.8289]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.matmul(tensor1,tensor2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvWqPoanv3r6",
        "outputId": "524eb810-a397-4fd9-a338-92b1fb29b5f6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.4095,  0.4509,  0.8631,  6.1494],\n",
              "        [ 0.0336,  1.4988,  0.3229, -0.5290],\n",
              "        [ 1.0589, -1.7170,  0.8910,  0.6088],\n",
              "        [-3.4194,  0.8273,  0.6376,  1.8289]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Concatination"
      ],
      "metadata": {
        "id": "xRXOF_KQv9Ju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t1 = torch.tensor([1,2,3])\n",
        "t2 = torch.tensor([4])\n",
        "torch.cat((t1,t2),dim=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysAgDpXvv6sk",
        "outputId": "130bea92-b0c4-4ce8-bde6-21039ceab7f1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 2, 3, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H85TpyQtwSsQ",
        "outputId": "c7a588be-71ec-4242-9011-19251663830b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.5052,  0.0696,  0.3811, -1.0432, -1.8870],\n",
              "        [-0.3648,  1.0843, -0.5075, -0.6726,  0.6484],\n",
              "        [-1.1391, -0.9684,  0.1949, -0.4022,  0.1019],\n",
              "        [-0.5731,  1.8145,  1.1503,  0.8639, -0.1164]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cat((tensor1,tensor1),dim=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8jsAY_UwChy",
        "outputId": "f440af85-b4e3-4e98-ac92-2810acb8f019"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.5052,  0.0696,  0.3811, -1.0432, -1.8870],\n",
              "        [-0.3648,  1.0843, -0.5075, -0.6726,  0.6484],\n",
              "        [-1.1391, -0.9684,  0.1949, -0.4022,  0.1019],\n",
              "        [-0.5731,  1.8145,  1.1503,  0.8639, -0.1164],\n",
              "        [-0.5052,  0.0696,  0.3811, -1.0432, -1.8870],\n",
              "        [-0.3648,  1.0843, -0.5075, -0.6726,  0.6484],\n",
              "        [-1.1391, -0.9684,  0.1949, -0.4022,  0.1019],\n",
              "        [-0.5731,  1.8145,  1.1503,  0.8639, -0.1164]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cat((tensor1,tensor1),dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mA9b_7izwEb-",
        "outputId": "7101c7fb-ae28-4c39-8f16-9c9b989121f9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.5052,  0.0696,  0.3811, -1.0432, -1.8870, -0.5052,  0.0696,  0.3811,\n",
              "         -1.0432, -1.8870],\n",
              "        [-0.3648,  1.0843, -0.5075, -0.6726,  0.6484, -0.3648,  1.0843, -0.5075,\n",
              "         -0.6726,  0.6484],\n",
              "        [-1.1391, -0.9684,  0.1949, -0.4022,  0.1019, -1.1391, -0.9684,  0.1949,\n",
              "         -0.4022,  0.1019],\n",
              "        [-0.5731,  1.8145,  1.1503,  0.8639, -0.1164, -0.5731,  1.8145,  1.1503,\n",
              "          0.8639, -0.1164]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.stack([tensor1,tensor1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vrEyberwPeO",
        "outputId": "bbc9b62d-869c-42d6-9e62-03d9f596b216"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.5052,  0.0696,  0.3811, -1.0432, -1.8870],\n",
              "         [-0.3648,  1.0843, -0.5075, -0.6726,  0.6484],\n",
              "         [-1.1391, -0.9684,  0.1949, -0.4022,  0.1019],\n",
              "         [-0.5731,  1.8145,  1.1503,  0.8639, -0.1164]],\n",
              "\n",
              "        [[-0.5052,  0.0696,  0.3811, -1.0432, -1.8870],\n",
              "         [-0.3648,  1.0843, -0.5075, -0.6726,  0.6484],\n",
              "         [-1.1391, -0.9684,  0.1949, -0.4022,  0.1019],\n",
              "         [-0.5731,  1.8145,  1.1503,  0.8639, -0.1164]]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### nn.Multinomial"
      ],
      "metadata": {
        "id": "OtbIkdUPw5kr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.multinomial(input, num_samples, replacement=False, *, generator=None, out=None) → LongTensor\n",
        "\n",
        "# Parameters:\n",
        "    # input (Tensor) – the input tensor containing probabilities\n",
        "\n",
        "    # num_samples (int) – number of samples to draw\n",
        "\n",
        "    # replacement (bool, optional) – whether to draw with replacement or not\n",
        "\n",
        "probabilities = torch.tensor([0.1, 0.4 , 0.5]) # 10% => 0, 40%=> 1 , 50% for 2 ,  each probability points to the index of the probability in the tensor\n",
        "\n",
        "samples = torch.multinomial(probabilities, num_samples=10, replacement=True)\n",
        "print(samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WmHjVXcwYgb",
        "outputId": "7d0938c4-df89-4361-fd95-68589fd0649a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 1, 1, 2, 1, 2, 2, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trail Matrix\n",
        "\n",
        "Returns the lower triangular part of the matrix (2-D tensor) or batch of matrices input, the other elements of the result tensor out are set to 0."
      ],
      "metadata": {
        "id": "4UUg-fOBxAkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tensor1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsqmD18Zw-ED",
        "outputId": "c921d413-47e2-403c-c87e-bb33cc247d7a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.5052,  0.0696,  0.3811, -1.0432, -1.8870],\n",
            "        [-0.3648,  1.0843, -0.5075, -0.6726,  0.6484],\n",
            "        [-1.1391, -0.9684,  0.1949, -0.4022,  0.1019],\n",
            "        [-0.5731,  1.8145,  1.1503,  0.8639, -0.1164]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.tril(tensor1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7TdNYHKxC0i",
        "outputId": "c68fea88-4f5d-46a7-fda9-c48ac60514f2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.5052,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.3648,  1.0843,  0.0000,  0.0000,  0.0000],\n",
            "        [-1.1391, -0.9684,  0.1949,  0.0000,  0.0000],\n",
            "        [-0.5731,  1.8145,  1.1503,  0.8639,  0.0000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-eZJSHrxEm6",
        "outputId": "9447fe31-fb4b-4bce-a177-97ff42d975c3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = tensor1.masked_fill(torch.tril(torch.ones(4, 5)) == 0, float('-inf'))\n",
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJGb7frXxHoe",
        "outputId": "b58a45ee-50ae-4bbe-cd29-685949f73c09"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.5052,    -inf,    -inf,    -inf,    -inf],\n",
              "        [-0.3648,  1.0843,    -inf,    -inf,    -inf],\n",
              "        [-1.1391, -0.9684,  0.1949,    -inf,    -inf],\n",
              "        [-0.5731,  1.8145,  1.1503,  0.8639,    -inf]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transpose"
      ],
      "metadata": {
        "id": "g1fI-tKwxM2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t3 = torch.rand(2,3,4)\n",
        "t3.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBEJFjItxJka",
        "outputId": "81b79bb4-ecb2-4ef2-c3f6-3e6635aeeada"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jv9fJ8d1xO-W",
        "outputId": "e72faf0b-4a1f-4416-fc35-97caa96b657e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.9387, 0.0717, 0.7192, 0.0798],\n",
              "         [0.5136, 0.3794, 0.2516, 0.1496],\n",
              "         [0.0974, 0.5239, 0.0963, 0.2048]],\n",
              "\n",
              "        [[0.1060, 0.1647, 0.8310, 0.5854],\n",
              "         [0.9952, 0.8251, 0.1153, 0.1378],\n",
              "         [0.3005, 0.8768, 0.2380, 0.1228]]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t3.transpose(0,1).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "306hu2GxxRJ0",
        "outputId": "a33c6cd2-1171-4558-cda3-9125058d26b5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 2, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### nn.Embedding"
      ],
      "metadata": {
        "id": "FOEXAxFExWSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an embedding layer\n",
        "vocab_size = 50\n",
        "embedding_dim = 100\n",
        "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "# Create some input indices\n",
        "input_indices = torch.LongTensor([1, 0, 5])\n",
        "\n",
        "# Apply the embedding layer\n",
        "embedded_output = embedding(input_indices)\n",
        "\n",
        "print(embedded_output.shape)  # no of tokens * embedding dimension"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2MS_6HGxTx5",
        "outputId": "21c0a0a5-61b4-437f-9d59-9d41fce320db"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedded_output[0].shape)"
      ],
      "metadata": {
        "id": "V-jZFM7HxYNs",
        "outputId": "d78aee16-2f2e-4234-f6e5-dd3bb199a78c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RegisterBuffer"
      ],
      "metadata": {
        "id": "SJ6-cXHCrv2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedHead(nn.Module):\n",
        "    \"\"\" One masked self attention head \"\"\"\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.w_query = nn.Linear(n_embd , head_size , bias=False)\n",
        "        self.w_key = nn.Linear(n_embd , head_size , bias=False)\n",
        "        self.w_value = nn.Linear(n_embd , head_size, bias=False)\n",
        "\n",
        "        #self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size)))\n",
        "        self.mask = torch.triu(torch.ones(block_size, block_size), diagonal=1)\n",
        "\n",
        "\n",
        "        self.dropout = nn.Dropout(drop_out)\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        print(f'MMH , x shape{x.shape}')\n",
        "\n",
        "        B,T,D = x.shape\n",
        "\n",
        "        q = self.w_query(x)\n",
        "        k = self.w_key(x)\n",
        "        v = self.w_value(x)\n",
        "\n",
        "        wei = q @ k.transpose(-2 , -1) * k.shape[-1]**-0.5\n",
        "        wei = wei.masked_fill(self.mask[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim = -1)\n",
        "        wei = self.dropout(wei)\n",
        "        out = wei @ v\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "yZpkEcmKxa1_"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(100)\n",
        "\n",
        "inputs = torch.rand(6,5,)\n",
        "\n",
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "block_size = batch.shape[1]\n",
        "n_embd = inputs.shape[1]\n",
        "\n",
        "head_size = 64\n",
        "\n",
        "drop_out = 0.1"
      ],
      "metadata": {
        "id": "Km5cC_huv3-W"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(n_embd)\n",
        "print(head_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDj08xzvyQI-",
        "outputId": "6e1e8dda-ebec-4dc7-c38c-80bee0219f21"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Masked_without_buffer = MaskedHead(head_size)\n",
        "\n",
        "with torch.no_grad():\n",
        "    context_vecs = Masked_without_buffer(batch)\n",
        "\n",
        "print(context_vecs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgcPbp38xvwN",
        "outputId": "b147138e-c7de-4193-b38e-3096dfbfc313"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MMH , x shapetorch.Size([2, 6, 5])\n",
            "tensor([[[-2.8405e-02,  1.4376e-01, -9.9015e-02, -1.8295e-01,  3.0590e-01,\n",
            "          -9.4967e-03,  6.7290e-02, -3.9946e-01, -5.7709e-01,  3.4139e-01,\n",
            "           1.6032e-02, -7.1096e-02,  5.6491e-01,  2.2933e-01,  1.6770e-01,\n",
            "           1.2671e-01,  1.7895e-01, -3.7584e-02, -5.3722e-02, -3.3519e-02,\n",
            "           1.0927e-02, -2.7048e-01,  5.5251e-02,  3.2146e-01, -8.0959e-03,\n",
            "          -8.2708e-02, -6.6841e-02, -1.9353e-01,  1.5847e-01,  1.1592e-01,\n",
            "           2.9076e-01, -9.7154e-02, -7.7488e-02,  1.8323e-01, -1.6009e-01,\n",
            "          -3.1393e-01,  9.9828e-02, -4.4461e-01,  4.0745e-01,  1.8787e-02,\n",
            "           6.7223e-02, -3.4792e-02, -5.5663e-02, -1.9362e-01, -2.8038e-01,\n",
            "          -9.9853e-02,  1.7589e-01, -1.6521e-01, -3.4372e-01, -2.6263e-01,\n",
            "          -1.1766e-01,  4.8308e-02,  5.4942e-02,  3.7899e-01, -3.9429e-01,\n",
            "           3.5823e-01, -1.0223e-01,  5.4895e-01, -3.0139e-01, -1.6646e-01,\n",
            "           2.8152e-01,  2.7729e-01,  9.7563e-02, -5.5784e-02],\n",
            "         [-6.4292e-02, -1.9445e-02, -1.4212e-01, -1.6073e-01,  2.4556e-01,\n",
            "          -2.3244e-02,  4.9283e-02, -2.6909e-01, -4.2614e-01,  9.9133e-02,\n",
            "           8.3267e-02, -7.4279e-02,  4.2334e-01,  1.4353e-01,  4.3885e-02,\n",
            "           1.1440e-01,  4.5616e-02, -6.8005e-02,  1.4426e-02, -2.6823e-02,\n",
            "           8.0143e-03, -1.2361e-01,  4.9689e-02,  3.0537e-01, -5.9499e-02,\n",
            "          -1.4542e-03, -5.7402e-02, -8.6614e-02,  1.5568e-01, -2.8802e-02,\n",
            "           1.4691e-01,  5.8493e-02, -4.0935e-03,  2.1510e-01, -1.3510e-01,\n",
            "          -2.4835e-01, -4.8858e-02, -3.2347e-01,  3.5368e-01,  1.3082e-01,\n",
            "           7.1879e-02,  3.4055e-02, -8.5608e-02, -1.1559e-01, -1.2177e-01,\n",
            "          -7.8112e-02,  2.2635e-01, -9.9989e-02, -1.4652e-01, -2.5223e-01,\n",
            "           5.6581e-02,  5.1051e-02,  3.1513e-02,  2.5105e-01, -1.5606e-01,\n",
            "           3.0112e-01, -7.3451e-02,  4.0971e-01, -1.2223e-01, -1.9068e-01,\n",
            "           2.1292e-01,  2.5063e-01, -3.5253e-02, -9.3142e-02],\n",
            "         [-1.0307e-01,  2.9895e-01, -8.7744e-02, -2.3976e-01,  3.9670e-01,\n",
            "           1.3534e-02,  3.3233e-02, -5.9098e-01, -8.7069e-01,  5.6304e-01,\n",
            "          -1.9041e-02, -1.2948e-01,  7.8844e-01,  2.8213e-01,  2.3512e-01,\n",
            "           1.2071e-01,  3.6197e-01, -1.1957e-01, -2.0158e-01, -5.9133e-02,\n",
            "           2.2739e-02, -4.5022e-01,  2.9186e-02,  4.8043e-01, -6.7047e-02,\n",
            "          -1.1812e-01, -5.2027e-02, -2.8043e-01,  2.5046e-01,  2.2317e-01,\n",
            "           4.4370e-01, -2.2220e-01, -1.3919e-01,  2.8361e-01, -2.4919e-01,\n",
            "          -4.5332e-01,  2.4226e-01, -6.3148e-01,  5.1430e-01, -3.9071e-02,\n",
            "           1.4605e-01, -1.1761e-01,  7.2309e-03, -2.9709e-01, -3.9831e-01,\n",
            "          -1.4843e-01,  1.7006e-01, -2.3951e-01, -5.2034e-01, -3.1076e-01,\n",
            "          -2.5995e-01,  9.7816e-02,  1.4373e-01,  5.5249e-01, -7.3402e-01,\n",
            "           4.8486e-01, -1.3619e-01,  7.7517e-01, -4.1969e-01, -1.5746e-01,\n",
            "           3.9136e-01,  4.0312e-01,  1.9217e-01, -1.3801e-01],\n",
            "         [-6.1380e-02,  1.6620e-01, -1.4334e-01, -1.8361e-01,  4.0328e-01,\n",
            "           2.3640e-02, -5.7596e-03, -5.2812e-01, -8.7428e-01,  3.7671e-01,\n",
            "           1.1383e-01, -2.1511e-01,  7.6843e-01,  2.5415e-01,  1.4260e-01,\n",
            "           1.7444e-01,  2.7678e-01, -1.4666e-01, -1.8197e-01, -1.1263e-01,\n",
            "          -2.4779e-02, -3.5133e-01, -9.1705e-03,  5.4356e-01, -7.6175e-02,\n",
            "          -7.5419e-02, -9.1550e-03, -2.2185e-01,  2.8349e-01,  1.0245e-01,\n",
            "           4.0574e-01, -7.5820e-02, -7.6200e-02,  3.5792e-01, -2.3020e-01,\n",
            "          -4.3083e-01,  1.5870e-01, -6.7999e-01,  5.3565e-01,  5.8364e-02,\n",
            "           1.3378e-01, -3.2404e-02, -5.6117e-02, -2.5597e-01, -3.0034e-01,\n",
            "          -1.8379e-01,  2.4231e-01, -2.6614e-01, -4.2095e-01, -3.3294e-01,\n",
            "          -8.7382e-02,  1.0802e-01,  1.3742e-01,  5.3479e-01, -6.0802e-01,\n",
            "           4.8001e-01, -2.3645e-01,  7.6278e-01, -3.0036e-01, -1.7297e-01,\n",
            "           3.7841e-01,  4.9357e-01,  7.5730e-02, -9.7999e-02],\n",
            "         [-5.7226e-02,  4.4059e-01,  1.4483e-02, -6.2707e-02,  2.9554e-01,\n",
            "           1.0515e-01, -9.2579e-02, -5.3864e-01, -8.7051e-01,  5.9612e-01,\n",
            "          -6.2054e-03, -2.2528e-01,  6.6386e-01,  1.9853e-01,  1.9169e-01,\n",
            "           7.4929e-02,  4.8194e-01, -1.4578e-01, -3.9856e-01, -1.6679e-01,\n",
            "          -4.8270e-02, -4.8772e-01, -1.5285e-01,  4.4279e-01, -6.9442e-02,\n",
            "          -1.3643e-01,  7.4995e-02, -2.3157e-01,  2.2458e-01,  3.2070e-01,\n",
            "           5.3216e-01, -2.9778e-01, -1.4456e-01,  2.5293e-01, -1.9257e-01,\n",
            "          -3.8464e-01,  4.2757e-01, -6.3690e-01,  3.2996e-01, -1.9561e-01,\n",
            "           1.2730e-01, -1.5500e-01,  1.4358e-01, -2.5618e-01, -3.4146e-01,\n",
            "          -2.1676e-01,  2.5294e-02, -3.2787e-01, -5.4661e-01, -1.1884e-01,\n",
            "          -3.5998e-01,  1.2849e-01,  2.0935e-01,  5.4828e-01, -9.6343e-01,\n",
            "           3.4893e-01, -2.6440e-01,  6.8782e-01, -3.7527e-01,  2.8209e-02,\n",
            "           3.0614e-01,  4.1548e-01,  2.5706e-01, -7.4646e-02],\n",
            "         [        nan,         nan,         nan,         nan,         nan,\n",
            "                  nan,         nan,         nan,         nan,         nan,\n",
            "                  nan,         nan,         nan,         nan,         nan,\n",
            "                  nan,         nan,         nan,         nan,         nan,\n",
            "                  nan,         nan,         nan,         nan,         nan,\n",
            "                  nan,         nan,         nan,         nan,         nan,\n",
            "                  nan,         nan,         nan,         nan,         nan,\n",
            "                  nan,         nan,         nan,         nan,         nan,\n",
            "                  nan,         nan,         nan,         nan,         nan,\n",
            "                  nan,         nan,         nan,         nan,         nan,\n",
            "                  nan,         nan,         nan,         nan,         nan,\n",
            "                  nan,         nan,         nan,         nan,         nan,\n",
            "                  nan,         nan,         nan,         nan]],\n",
            "\n",
            "        [[-3.9531e-02,  2.2942e-01, -9.6199e-02, -1.9514e-01,  3.6336e-01,\n",
            "           1.0947e-02,  4.9291e-02, -5.0418e-01, -7.4633e-01,  4.5729e-01,\n",
            "           1.4825e-02, -1.1489e-01,  6.9398e-01,  2.6793e-01,  2.0497e-01,\n",
            "           1.4127e-01,  2.7265e-01, -6.5926e-02, -1.3121e-01, -6.5947e-02,\n",
            "           1.5428e-03, -3.6530e-01,  2.5533e-02,  4.0754e-01, -2.1597e-02,\n",
            "          -1.0923e-01, -5.2260e-02, -2.3856e-01,  2.0213e-01,  1.7826e-01,\n",
            "           3.9422e-01, -1.5505e-01, -1.0559e-01,  2.3240e-01, -1.9753e-01,\n",
            "          -3.8872e-01,  1.8296e-01, -5.6844e-01,  4.7160e-01, -1.9242e-02,\n",
            "           9.1972e-02, -6.4928e-02, -2.7748e-02, -2.4343e-01, -3.4677e-01,\n",
            "          -1.4200e-01,  1.8081e-01, -2.2895e-01, -4.4999e-01, -2.8574e-01,\n",
            "          -1.8765e-01,  7.3290e-02,  9.5645e-02,  4.8558e-01, -5.8160e-01,\n",
            "           4.2607e-01, -1.5363e-01,  6.8267e-01, -3.7435e-01, -1.6097e-01,\n",
            "           3.4104e-01,  3.5806e-01,  1.4754e-01, -7.0297e-02],\n",
            "         [-1.2656e-01,  2.3420e-01, -1.3198e-01, -2.6704e-01,  4.1595e-01,\n",
            "           5.2001e-04,  5.5744e-02, -5.8497e-01, -8.5979e-01,  4.8658e-01,\n",
            "           7.1771e-03, -1.1745e-01,  7.9794e-01,  2.7900e-01,  1.9947e-01,\n",
            "           1.3547e-01,  3.0057e-01, -1.1999e-01, -1.4463e-01, -5.4545e-02,\n",
            "           2.7033e-02, -4.0983e-01,  4.0109e-02,  5.0343e-01, -8.8835e-02,\n",
            "          -8.7393e-02, -7.5006e-02, -2.4574e-01,  2.5736e-01,  1.6989e-01,\n",
            "           4.1031e-01, -1.4783e-01, -1.0807e-01,  3.1054e-01, -2.5594e-01,\n",
            "          -4.7022e-01,  1.6125e-01, -6.1525e-01,  5.5491e-01,  2.2082e-02,\n",
            "           1.4682e-01, -7.8571e-02, -1.5585e-02, -2.7582e-01, -3.5858e-01,\n",
            "          -1.5044e-01,  2.3831e-01, -2.2734e-01, -4.6534e-01, -3.4935e-01,\n",
            "          -1.8832e-01,  1.0210e-01,  1.2269e-01,  5.3570e-01, -6.4516e-01,\n",
            "           5.1314e-01, -1.2019e-01,  7.8274e-01, -3.8396e-01, -2.1613e-01,\n",
            "           3.9476e-01,  4.0786e-01,  1.3774e-01, -1.6775e-01],\n",
            "         [-1.0307e-01,  2.9895e-01, -8.7744e-02, -2.3976e-01,  3.9670e-01,\n",
            "           1.3534e-02,  3.3233e-02, -5.9098e-01, -8.7069e-01,  5.6304e-01,\n",
            "          -1.9041e-02, -1.2948e-01,  7.8844e-01,  2.8213e-01,  2.3512e-01,\n",
            "           1.2071e-01,  3.6197e-01, -1.1957e-01, -2.0158e-01, -5.9133e-02,\n",
            "           2.2739e-02, -4.5022e-01,  2.9186e-02,  4.8043e-01, -6.7047e-02,\n",
            "          -1.1812e-01, -5.2027e-02, -2.8043e-01,  2.5046e-01,  2.2317e-01,\n",
            "           4.4370e-01, -2.2220e-01, -1.3919e-01,  2.8361e-01, -2.4919e-01,\n",
            "          -4.5332e-01,  2.4226e-01, -6.3148e-01,  5.1430e-01, -3.9071e-02,\n",
            "           1.4605e-01, -1.1761e-01,  7.2309e-03, -2.9709e-01, -3.9831e-01,\n",
            "          -1.4843e-01,  1.7006e-01, -2.3951e-01, -5.2034e-01, -3.1076e-01,\n",
            "          -2.5995e-01,  9.7816e-02,  1.4373e-01,  5.5249e-01, -7.3402e-01,\n",
            "           4.8486e-01, -1.3619e-01,  7.7517e-01, -4.1969e-01, -1.5746e-01,\n",
            "           3.9136e-01,  4.0312e-01,  1.9217e-01, -1.3801e-01],\n",
            "         [-2.7691e-02,  2.1320e-01,  7.0081e-03, -3.0344e-02,  1.4301e-01,\n",
            "           5.0882e-02, -4.4799e-02, -2.6064e-01, -4.2124e-01,  2.8846e-01,\n",
            "          -3.0028e-03, -1.0901e-01,  3.2124e-01,  9.6069e-02,  9.2759e-02,\n",
            "           3.6258e-02,  2.3321e-01, -7.0542e-02, -1.9286e-01, -8.0711e-02,\n",
            "          -2.3358e-02, -2.3601e-01, -7.3966e-02,  2.1426e-01, -3.3603e-02,\n",
            "          -6.6018e-02,  3.6290e-02, -1.1206e-01,  1.0867e-01,  1.5518e-01,\n",
            "           2.5751e-01, -1.4409e-01, -6.9953e-02,  1.2239e-01, -9.3183e-02,\n",
            "          -1.8612e-01,  2.0690e-01, -3.0820e-01,  1.5967e-01, -9.4653e-02,\n",
            "           6.1600e-02, -7.5006e-02,  6.9480e-02, -1.2396e-01, -1.6523e-01,\n",
            "          -1.0489e-01,  1.2240e-02, -1.5866e-01, -2.6450e-01, -5.7509e-02,\n",
            "          -1.7420e-01,  6.2178e-02,  1.0131e-01,  2.6531e-01, -4.6620e-01,\n",
            "           1.6884e-01, -1.2794e-01,  3.3283e-01, -1.8159e-01,  1.3650e-02,\n",
            "           1.4814e-01,  2.0105e-01,  1.2439e-01, -3.6121e-02],\n",
            "         [-5.7226e-02,  4.4059e-01,  1.4483e-02, -6.2707e-02,  2.9554e-01,\n",
            "           1.0515e-01, -9.2579e-02, -5.3864e-01, -8.7051e-01,  5.9612e-01,\n",
            "          -6.2054e-03, -2.2528e-01,  6.6386e-01,  1.9853e-01,  1.9169e-01,\n",
            "           7.4929e-02,  4.8194e-01, -1.4578e-01, -3.9856e-01, -1.6679e-01,\n",
            "          -4.8270e-02, -4.8772e-01, -1.5285e-01,  4.4279e-01, -6.9442e-02,\n",
            "          -1.3643e-01,  7.4995e-02, -2.3157e-01,  2.2458e-01,  3.2070e-01,\n",
            "           5.3216e-01, -2.9778e-01, -1.4456e-01,  2.5293e-01, -1.9257e-01,\n",
            "          -3.8464e-01,  4.2757e-01, -6.3690e-01,  3.2996e-01, -1.9561e-01,\n",
            "           1.2730e-01, -1.5500e-01,  1.4358e-01, -2.5618e-01, -3.4146e-01,\n",
            "          -2.1676e-01,  2.5294e-02, -3.2787e-01, -5.4661e-01, -1.1884e-01,\n",
            "          -3.5998e-01,  1.2849e-01,  2.0935e-01,  5.4828e-01, -9.6343e-01,\n",
            "           3.4893e-01, -2.6440e-01,  6.8782e-01, -3.7527e-01,  2.8209e-02,\n",
            "           3.0614e-01,  4.1548e-01,  2.5706e-01, -7.4646e-02],\n",
            "         [        nan,         nan,         nan,         nan,         nan,\n",
            "                  nan,         nan,         nan,         nan,         nan,\n",
            "                  nan,         nan,         nan,         nan,         nan,\n",
            "                  nan,         nan,         nan,         nan,         nan,\n",
            "                  nan,         nan,         nan,         nan,         nan,\n",
            "                  nan,         nan,         nan,         nan,         nan,\n",
            "                  nan,         nan,         nan,         nan,         nan,\n",
            "                  nan,         nan,         nan,         nan,         nan,\n",
            "                  nan,         nan,         nan,         nan,         nan,\n",
            "                  nan,         nan,         nan,         nan,         nan,\n",
            "                  nan,         nan,         nan,         nan,         nan,\n",
            "                  nan,         nan,         nan,         nan,         nan,\n",
            "                  nan,         nan,         nan,         nan]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Move the model and batch to GPU\n",
        "\n",
        "batch = batch.to(device)\n",
        "Masked_without_buffer.to(device);\n",
        "\n",
        "with torch.no_grad():\n",
        "    context_vecs = Masked_without_buffer(batch)\n",
        "\n",
        "print(context_vecs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "DAunMBtbyiwS",
        "outputId": "02882fd8-e29c-4ebd-853b-9e59d877be9b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MMH , x shapetorch.Size([2, 6, 5])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "expected self and mask to be on the same device, but got mask on cpu and self on cuda:0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-3946a4d0af8a>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mcontext_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMasked_without_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_vecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-045026ed637b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mwei\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mwei\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwei\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mwei\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwei\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mwei\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwei\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: expected self and mask to be on the same device, but got mask on cpu and self on cuda:0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"w_query's device:\", Masked_without_buffer.w_query.weight.device)\n",
        "print(\"w_key's device:\", Masked_without_buffer.w_key.weight.device)\n",
        "print(\"w_value's device:\", Masked_without_buffer.w_value.weight.device)\n",
        "print(\"mask's device:\", Masked_without_buffer.mask.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2s-LT94EGjB",
        "outputId": "84b402d6-c9e4-4ff8-94a8-ffbfb3c28513"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w_query's device: cuda:0\n",
            "w_key's device: cuda:0\n",
            "w_value's device: cuda:0\n",
            "mask's device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedHead(nn.Module):\n",
        "    \"\"\" One masked self attention head \"\"\"\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.w_query = nn.Linear(n_embd , head_size , bias=False)\n",
        "        self.w_key = nn.Linear(n_embd , head_size , bias=False)\n",
        "        self.w_value = nn.Linear(n_embd , head_size, bias=False)\n",
        "\n",
        "        self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size)))\n",
        "        #self.mask = torch.triu(torch.ones(block_size, block_size), diagonal=1)\n",
        "\n",
        "\n",
        "        self.dropout = nn.Dropout(drop_out)\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        print(f'MMH , x shape{x.shape}')\n",
        "\n",
        "        B,T,D = x.shape\n",
        "\n",
        "        q = self.w_query(x)\n",
        "        k = self.w_key(x)\n",
        "        v = self.w_value(x)\n",
        "\n",
        "        wei = q @ k.transpose(-2 , -1) * k.shape[-1]**-0.5\n",
        "        #wei = wei.masked_fill(self.mask[:T, :T] == 0, float('-inf'))\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim = -1)\n",
        "        wei = self.dropout(wei)\n",
        "        out = wei @ v\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "QHtF84zbEi3G"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Initialize the model again\n",
        "\n",
        "Masked_with_buffer = MaskedHead(head_size)\n",
        "\n",
        "### Move the model and batch to GPU\n",
        "\n",
        "batch = batch.to(device)\n",
        "Masked_with_buffer.to(device);\n",
        "\n",
        "with torch.no_grad():\n",
        "    context_vecs = Masked_with_buffer(batch)\n",
        "\n",
        "print(context_vecs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eaw3qGxsFHJi",
        "outputId": "c436399d-7ac0-48f9-ed28-2f3a6a14f183"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MMH , x shapetorch.Size([2, 6, 5])\n",
            "tensor([[[-1.2173e-01, -2.3865e-01, -2.2987e-01,  2.0274e-01, -1.7669e-01,\n",
            "           1.4680e-01, -4.8870e-01,  4.9394e-02, -2.8143e-01, -2.7290e-02,\n",
            "          -2.6407e-01, -1.1670e-01,  2.9807e-01, -2.3353e-01, -3.2353e-01,\n",
            "           3.8558e-01,  5.7987e-01, -3.7832e-02, -1.0542e-01,  7.7300e-01,\n",
            "           2.0348e-01, -9.5054e-01,  1.2327e-01,  2.3186e-01, -3.9382e-01,\n",
            "           9.2293e-02,  5.8114e-02,  1.4911e-01,  3.4933e-01, -1.8365e-01,\n",
            "          -3.5265e-02, -1.3517e-01,  7.7951e-01,  4.3207e-01,  3.6084e-01,\n",
            "           2.0901e-02,  6.2128e-01, -5.1342e-01, -5.6163e-01, -1.5253e-01,\n",
            "          -2.7629e-02,  1.6029e-03, -2.6663e-01, -2.0503e-01,  2.7986e-01,\n",
            "          -2.5866e-01,  1.2962e-01, -3.1527e-01, -2.2545e-01,  9.1799e-01,\n",
            "          -3.1127e-01, -2.4858e-02,  8.6299e-02,  3.2788e-01,  2.5509e-01,\n",
            "           5.9667e-01,  1.7422e-02, -4.4825e-03, -2.6015e-01,  8.8944e-02,\n",
            "           2.1821e-01,  8.7226e-01, -1.7555e-01, -2.1152e-01],\n",
            "         [ 1.1647e-01, -2.3200e-01, -3.4955e-02,  1.3530e-01, -1.5083e-01,\n",
            "           8.2923e-02, -4.6720e-01,  5.2635e-02, -2.6778e-01, -6.7324e-02,\n",
            "          -2.2831e-01,  1.2366e-01,  2.7027e-01, -2.1534e-01, -2.8339e-02,\n",
            "           3.1576e-01,  2.3852e-01,  7.2081e-02, -5.3588e-02,  3.2815e-01,\n",
            "           2.8823e-01, -6.6464e-01,  6.1542e-02,  3.2401e-01, -4.3494e-02,\n",
            "           1.6779e-01, -5.2875e-02,  3.8703e-02,  1.7476e-01, -2.8922e-01,\n",
            "          -1.6859e-01,  2.9290e-02,  3.2268e-01,  2.6578e-01,  1.9704e-01,\n",
            "          -9.4045e-02,  2.3699e-01, -4.0409e-01, -2.8209e-01,  9.3628e-02,\n",
            "           3.4100e-02,  1.0332e-01, -5.3456e-02, -2.0865e-01,  1.2353e-01,\n",
            "          -2.7470e-01, -2.7730e-02, -1.3850e-01, -1.6477e-01,  5.5690e-01,\n",
            "          -1.3971e-02,  1.0914e-01,  8.6880e-02,  1.7133e-01,  1.3024e-01,\n",
            "           4.3833e-01,  1.4137e-01,  1.1278e-01, -2.9790e-01,  1.2980e-01,\n",
            "          -1.6771e-02,  4.8269e-01, -2.3776e-01, -4.7376e-02],\n",
            "         [ 1.5076e-02, -3.1669e-01, -1.1768e-01,  1.2712e-01, -1.6873e-01,\n",
            "           1.1481e-01, -5.0524e-01,  4.9214e-02, -2.2045e-01,  3.1073e-02,\n",
            "          -3.0018e-01,  7.4187e-02,  2.1615e-01, -1.5694e-01, -7.1365e-02,\n",
            "           3.4280e-01,  2.8960e-01,  9.7613e-02,  6.1346e-03,  4.8492e-01,\n",
            "           1.9880e-01, -8.3162e-01,  2.7701e-03,  3.3833e-01, -1.5294e-01,\n",
            "           1.4567e-01,  1.1066e-02,  5.6058e-02,  2.2426e-01, -1.9543e-01,\n",
            "          -9.8888e-02, -2.3870e-02,  5.1563e-01,  1.9396e-01,  2.4484e-01,\n",
            "          -8.1019e-02,  3.6183e-01, -4.6496e-01, -3.3731e-01,  1.5286e-02,\n",
            "          -3.6599e-02,  1.1091e-02, -1.2811e-01, -2.9191e-01,  1.4800e-01,\n",
            "          -2.2074e-01,  6.5103e-02, -1.4501e-01, -1.5818e-01,  6.9854e-01,\n",
            "          -3.5161e-02,  2.8419e-02,  9.7451e-02,  2.0055e-01,  2.8965e-01,\n",
            "           4.1184e-01,  1.0217e-01,  1.6745e-01, -2.1633e-01,  2.0164e-02,\n",
            "          -7.7296e-03,  6.7042e-01, -2.0366e-01, -1.4351e-01],\n",
            "         [ 1.9151e-02, -2.7071e-01, -1.0218e-01,  1.7542e-01, -2.2866e-01,\n",
            "           1.0594e-01, -4.7866e-01,  8.1415e-02, -1.4857e-01,  5.3893e-02,\n",
            "          -3.4772e-01, -4.3866e-02,  1.8574e-01, -9.1483e-02, -2.7486e-02,\n",
            "           3.0811e-01,  3.0836e-01,  1.0865e-01, -1.6492e-02,  4.4787e-01,\n",
            "           1.6421e-01, -7.9041e-01, -7.1056e-02,  3.6452e-01, -2.5244e-01,\n",
            "           1.4183e-01,  1.3091e-01,  1.0713e-01,  3.0900e-01, -1.7537e-01,\n",
            "          -6.0843e-03, -5.4834e-02,  5.1347e-01,  1.9239e-01,  2.9232e-01,\n",
            "          -3.7346e-02,  3.9585e-01, -4.6363e-01, -3.7952e-01, -6.0410e-02,\n",
            "          -1.3802e-01, -9.5525e-03, -2.2098e-01, -2.2595e-01,  7.9107e-02,\n",
            "          -1.6899e-01,  1.2449e-01, -9.3256e-02, -1.8417e-01,  6.6009e-01,\n",
            "          -8.6900e-02, -8.9101e-02,  3.3668e-02,  2.3261e-01,  2.7169e-01,\n",
            "           2.9537e-01,  4.9551e-02,  1.5997e-01, -1.9369e-01, -1.7211e-02,\n",
            "          -4.2133e-03,  6.5005e-01, -1.8070e-01, -2.1906e-01],\n",
            "         [ 5.1190e-02, -3.6727e-01, -1.2274e-01,  1.5311e-01, -2.4028e-01,\n",
            "           1.2892e-01, -5.9905e-01,  8.4031e-02, -2.2974e-01,  6.3389e-02,\n",
            "          -4.0509e-01,  5.7746e-02,  2.4307e-01, -1.4202e-01, -2.2647e-02,\n",
            "           3.9584e-01,  3.1717e-01,  1.5814e-01,  2.5864e-02,  5.3330e-01,\n",
            "           2.1280e-01, -9.7774e-01, -6.2632e-02,  4.5214e-01, -2.0198e-01,\n",
            "           1.8834e-01,  7.0362e-02,  7.4480e-02,  2.9895e-01, -2.2835e-01,\n",
            "          -8.1242e-02, -3.4840e-02,  5.9076e-01,  1.9277e-01,  2.9928e-01,\n",
            "          -8.6659e-02,  4.2091e-01, -5.5482e-01, -3.9528e-01,  7.0983e-04,\n",
            "          -1.1090e-01, -2.4925e-03, -1.8192e-01, -3.2647e-01,  1.2984e-01,\n",
            "          -2.4701e-01,  1.0477e-01, -1.3093e-01, -1.9273e-01,  8.0131e-01,\n",
            "          -3.0529e-02, -2.6620e-02,  9.9301e-02,  2.4169e-01,  3.4872e-01,\n",
            "           4.1377e-01,  1.2957e-01,  2.3253e-01, -2.4423e-01, -8.9335e-03,\n",
            "          -4.6614e-02,  7.7965e-01, -2.2794e-01, -2.0863e-01],\n",
            "         [ 1.2530e-01, -3.0360e-01, -3.0887e-02,  1.9931e-01, -2.3427e-01,\n",
            "           9.9738e-02, -5.6830e-01,  8.1857e-02, -2.1868e-01, -1.0955e-02,\n",
            "          -3.4644e-01,  7.5757e-02,  2.4893e-01, -1.6439e-01,  4.5067e-02,\n",
            "           3.5001e-01,  2.5476e-01,  1.2749e-01, -4.5329e-02,  3.5304e-01,\n",
            "           2.9178e-01, -8.0500e-01, -3.3788e-02,  4.3628e-01, -1.2304e-01,\n",
            "           1.9866e-01,  5.2055e-02,  8.1308e-02,  2.7285e-01, -2.9736e-01,\n",
            "          -1.1171e-01,  2.1523e-02,  3.9959e-01,  2.3725e-01,  2.7671e-01,\n",
            "          -1.0165e-01,  3.0143e-01, -5.0516e-01, -3.4829e-01,  6.1871e-02,\n",
            "          -6.4553e-02,  7.8072e-02, -1.4250e-01, -2.6154e-01,  6.1793e-02,\n",
            "          -2.4413e-01,  3.5563e-02, -7.5610e-02, -2.0552e-01,  6.4159e-01,\n",
            "           2.0789e-03,  1.9547e-02,  4.4677e-02,  2.1510e-01,  2.1815e-01,\n",
            "           3.7223e-01,  1.0954e-01,  1.8945e-01, -2.9661e-01,  6.5438e-02,\n",
            "          -6.5106e-02,  5.9056e-01, -2.7420e-01, -1.4032e-01]],\n",
            "\n",
            "        [[-1.2173e-01, -2.3865e-01, -2.2987e-01,  2.0274e-01, -1.7669e-01,\n",
            "           1.4680e-01, -4.8870e-01,  4.9394e-02, -2.8143e-01, -2.7290e-02,\n",
            "          -2.6407e-01, -1.1670e-01,  2.9807e-01, -2.3353e-01, -3.2353e-01,\n",
            "           3.8558e-01,  5.7987e-01, -3.7832e-02, -1.0542e-01,  7.7300e-01,\n",
            "           2.0348e-01, -9.5054e-01,  1.2327e-01,  2.3186e-01, -3.9382e-01,\n",
            "           9.2293e-02,  5.8114e-02,  1.4911e-01,  3.4933e-01, -1.8365e-01,\n",
            "          -3.5265e-02, -1.3517e-01,  7.7951e-01,  4.3207e-01,  3.6084e-01,\n",
            "           2.0901e-02,  6.2128e-01, -5.1342e-01, -5.6163e-01, -1.5253e-01,\n",
            "          -2.7629e-02,  1.6029e-03, -2.6663e-01, -2.0503e-01,  2.7986e-01,\n",
            "          -2.5866e-01,  1.2962e-01, -3.1527e-01, -2.2545e-01,  9.1799e-01,\n",
            "          -3.1127e-01, -2.4858e-02,  8.6299e-02,  3.2788e-01,  2.5509e-01,\n",
            "           5.9667e-01,  1.7422e-02, -4.4825e-03, -2.6015e-01,  8.8944e-02,\n",
            "           2.1821e-01,  8.7226e-01, -1.7555e-01, -2.1152e-01],\n",
            "         [ 1.1647e-01, -2.3200e-01, -3.4955e-02,  1.3530e-01, -1.5083e-01,\n",
            "           8.2923e-02, -4.6720e-01,  5.2635e-02, -2.6778e-01, -6.7324e-02,\n",
            "          -2.2831e-01,  1.2366e-01,  2.7027e-01, -2.1534e-01, -2.8339e-02,\n",
            "           3.1576e-01,  2.3852e-01,  7.2081e-02, -5.3588e-02,  3.2815e-01,\n",
            "           2.8823e-01, -6.6464e-01,  6.1542e-02,  3.2401e-01, -4.3494e-02,\n",
            "           1.6779e-01, -5.2875e-02,  3.8703e-02,  1.7476e-01, -2.8922e-01,\n",
            "          -1.6859e-01,  2.9290e-02,  3.2268e-01,  2.6578e-01,  1.9704e-01,\n",
            "          -9.4045e-02,  2.3699e-01, -4.0409e-01, -2.8209e-01,  9.3628e-02,\n",
            "           3.4100e-02,  1.0332e-01, -5.3456e-02, -2.0865e-01,  1.2353e-01,\n",
            "          -2.7470e-01, -2.7730e-02, -1.3850e-01, -1.6477e-01,  5.5690e-01,\n",
            "          -1.3971e-02,  1.0914e-01,  8.6880e-02,  1.7133e-01,  1.3024e-01,\n",
            "           4.3833e-01,  1.4137e-01,  1.1278e-01, -2.9790e-01,  1.2980e-01,\n",
            "          -1.6771e-02,  4.8269e-01, -2.3776e-01, -4.7376e-02],\n",
            "         [ 1.5076e-02, -3.1669e-01, -1.1768e-01,  1.2712e-01, -1.6873e-01,\n",
            "           1.1481e-01, -5.0524e-01,  4.9214e-02, -2.2045e-01,  3.1073e-02,\n",
            "          -3.0018e-01,  7.4187e-02,  2.1615e-01, -1.5694e-01, -7.1365e-02,\n",
            "           3.4280e-01,  2.8960e-01,  9.7613e-02,  6.1346e-03,  4.8492e-01,\n",
            "           1.9880e-01, -8.3162e-01,  2.7701e-03,  3.3833e-01, -1.5294e-01,\n",
            "           1.4567e-01,  1.1066e-02,  5.6058e-02,  2.2426e-01, -1.9543e-01,\n",
            "          -9.8888e-02, -2.3870e-02,  5.1563e-01,  1.9396e-01,  2.4484e-01,\n",
            "          -8.1019e-02,  3.6183e-01, -4.6496e-01, -3.3731e-01,  1.5286e-02,\n",
            "          -3.6599e-02,  1.1091e-02, -1.2811e-01, -2.9191e-01,  1.4800e-01,\n",
            "          -2.2074e-01,  6.5103e-02, -1.4501e-01, -1.5818e-01,  6.9854e-01,\n",
            "          -3.5161e-02,  2.8419e-02,  9.7451e-02,  2.0055e-01,  2.8965e-01,\n",
            "           4.1184e-01,  1.0217e-01,  1.6745e-01, -2.1633e-01,  2.0164e-02,\n",
            "          -7.7296e-03,  6.7042e-01, -2.0366e-01, -1.4351e-01],\n",
            "         [ 1.1267e-01, -3.3503e-01, -6.2725e-02,  1.9682e-01, -2.6502e-01,\n",
            "           1.1334e-01, -6.0615e-01,  9.7213e-02, -2.2130e-01,  2.4590e-02,\n",
            "          -4.0361e-01,  5.2243e-02,  2.5559e-01, -1.4815e-01,  3.9271e-02,\n",
            "           3.8025e-01,  2.8971e-01,  1.5702e-01, -1.8601e-02,  4.2848e-01,\n",
            "           2.6769e-01, -9.0705e-01, -6.9191e-02,  4.8006e-01, -1.7602e-01,\n",
            "           2.0870e-01,  8.7731e-02,  9.0159e-02,  3.1448e-01, -2.8442e-01,\n",
            "          -8.7846e-02, -4.8149e-03,  4.8949e-01,  2.2586e-01,  3.0686e-01,\n",
            "          -9.3239e-02,  3.6587e-01, -5.5089e-01, -3.8893e-01,  2.8622e-02,\n",
            "          -1.1267e-01,  4.5626e-02, -1.8215e-01, -2.8623e-01,  7.4632e-02,\n",
            "          -2.5122e-01,  7.6715e-02, -8.7856e-02, -2.1567e-01,  7.2702e-01,\n",
            "          -1.5519e-02, -2.4083e-02,  5.8534e-02,  2.4168e-01,  2.7711e-01,\n",
            "           3.7995e-01,  1.2119e-01,  2.2178e-01, -2.8801e-01,  3.0076e-02,\n",
            "          -6.8544e-02,  6.8864e-01, -2.6412e-01, -1.9094e-01],\n",
            "         [ 5.1190e-02, -3.6727e-01, -1.2274e-01,  1.5311e-01, -2.4028e-01,\n",
            "           1.2892e-01, -5.9905e-01,  8.4031e-02, -2.2974e-01,  6.3389e-02,\n",
            "          -4.0509e-01,  5.7746e-02,  2.4307e-01, -1.4202e-01, -2.2647e-02,\n",
            "           3.9584e-01,  3.1717e-01,  1.5814e-01,  2.5864e-02,  5.3330e-01,\n",
            "           2.1280e-01, -9.7774e-01, -6.2632e-02,  4.5214e-01, -2.0198e-01,\n",
            "           1.8834e-01,  7.0362e-02,  7.4480e-02,  2.9895e-01, -2.2835e-01,\n",
            "          -8.1242e-02, -3.4840e-02,  5.9076e-01,  1.9277e-01,  2.9928e-01,\n",
            "          -8.6659e-02,  4.2091e-01, -5.5482e-01, -3.9528e-01,  7.0983e-04,\n",
            "          -1.1090e-01, -2.4925e-03, -1.8192e-01, -3.2647e-01,  1.2984e-01,\n",
            "          -2.4701e-01,  1.0477e-01, -1.3093e-01, -1.9273e-01,  8.0131e-01,\n",
            "          -3.0529e-02, -2.6620e-02,  9.9301e-02,  2.4169e-01,  3.4872e-01,\n",
            "           4.1377e-01,  1.2957e-01,  2.3253e-01, -2.4423e-01, -8.9335e-03,\n",
            "          -4.6614e-02,  7.7965e-01, -2.2794e-01, -2.0863e-01],\n",
            "         [ 9.3454e-02, -3.8398e-01, -7.8076e-02,  2.1726e-01, -2.6786e-01,\n",
            "           1.2926e-01, -6.6362e-01,  8.8728e-02, -2.3860e-01,  2.7464e-02,\n",
            "          -4.1977e-01,  7.1158e-02,  2.6584e-01, -1.7018e-01,  1.8924e-02,\n",
            "           4.1508e-01,  3.1922e-01,  1.5211e-01, -2.4064e-02,  4.8518e-01,\n",
            "           2.9392e-01, -9.9721e-01, -5.3281e-02,  4.9636e-01, -1.8499e-01,\n",
            "           2.1494e-01,  7.5506e-02,  9.6306e-02,  3.2617e-01, -2.9748e-01,\n",
            "          -1.0425e-01, -3.4701e-04,  5.4901e-01,  2.4450e-01,  3.3281e-01,\n",
            "          -1.1049e-01,  4.0268e-01, -6.0163e-01, -4.2196e-01,  3.7876e-02,\n",
            "          -9.4477e-02,  4.8642e-02, -1.8865e-01, -3.3740e-01,  9.4188e-02,\n",
            "          -2.6185e-01,  7.7613e-02, -1.0143e-01, -2.2908e-01,  8.0347e-01,\n",
            "          -1.0754e-02, -3.1452e-03,  6.4161e-02,  2.5768e-01,  3.1942e-01,\n",
            "           4.3039e-01,  1.1295e-01,  2.3534e-01, -3.0426e-01,  3.1568e-02,\n",
            "          -6.3371e-02,  7.6347e-01, -2.9589e-01, -1.9628e-01]]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    }
  ]
}